#!/usr/bin/env python3
"""
AIè®­ç»ƒæ•°æ®ç”Ÿæˆå™¨
å°†æ¯”èµ›æ•°æ® + ç¤¾åŒºè¯„è®ºè½¬æ¢ä¸ºAIè®­ç»ƒæ–‡æœ¬
"""

import sys
import os
import json
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

# æ·»åŠ backendè·¯å¾„åˆ°sys.path
backend_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'backend', 'python')
sys.path.append(backend_path)

# å¯¼å…¥åç«¯æœåŠ¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
try:
    from services.unified_data_service import UnifiedDataService
    from models.match import Match
    from models.comment import Comment
    from config.database import db
    BACKEND_AVAILABLE = True
except ImportError as e:
    print(f"åç«¯æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    BACKEND_AVAILABLE = False

@dataclass
class MatchData:
    """æ¯”èµ›æ•°æ®ç»“æ„"""
    match_id: str
    teams: str
    duration: str
    winner: str
    game_mode: str
    lobby_type: str
    start_time: datetime
    first_blood_time: int
    game_version: str
    region: str
    series_id: Optional[str] = None
    series_type: Optional[str] = None
    league_id: Optional[str] = None
    
@dataclass
class CommentData:
    """è¯„è®ºæ•°æ®ç»“æ„"""
    id: str
    content: str
    author: str
    created_at: datetime
    likes: int
    is_featured: bool
    analysis_quality: Optional[str] = None

class AITrainingDataGenerator:
    """AIè®­ç»ƒæ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self, use_backend: bool = True):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            use_backend: æ˜¯å¦ä½¿ç”¨åç«¯æ•°æ®åº“ï¼Œå¦‚æœä¸ºFalseåˆ™ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        """
        self.use_backend = use_backend and BACKEND_AVAILABLE
        if self.use_backend:
            self.unified_service = UnifiedDataService()
    
    def get_match_comprehensive_data(self, match_id: str) -> Dict[str, Any]:
        """è·å–æ¯”èµ›çš„ç»¼åˆæ•°æ®ï¼ˆå››æºæ•°æ®èšåˆï¼‰"""
        if self.use_backend:
            return self._get_match_data_from_backend(match_id)
        else:
            return self._get_mock_match_data(match_id)
    
    def get_featured_comments(self, match_id: str, min_likes: int = 5) -> List[CommentData]:
        """è·å–æ¯”èµ›çš„ç²¾é€‰è¯„è®º"""
        if self.use_backend:
            return self._get_comments_from_backend(match_id, min_likes)
        else:
            return self._get_mock_comments(match_id)
    
    def generate_training_sample(self, match_id: str) -> str:
        """ç”Ÿæˆå•ä¸ªæ¯”èµ›çš„è®­ç»ƒæ ·æœ¬"""
        try:
            # 1. è·å–æ¯”èµ›çš„å››æºæ•°æ®
            match_data = self.get_match_comprehensive_data(match_id)
            
            # 2. è·å–ç²¾é€‰è¯„è®º
            featured_comments = self.get_featured_comments(match_id)
            
            # 3. ç”Ÿæˆè®­ç»ƒæ–‡æœ¬
            training_text = self._format_training_text(match_data, featured_comments)
            
            return training_text
            
        except Exception as e:
            print(f"ç”Ÿæˆè®­ç»ƒæ ·æœ¬å¤±è´¥ {match_id}: {e}")
            return self._generate_error_sample(match_id, str(e))
    
    def generate_batch_training_data(self, match_ids: List[str], output_file: str = None) -> List[Dict[str, Any]]:
        """æ‰¹é‡ç”Ÿæˆè®­ç»ƒæ•°æ®"""
        training_samples = []
        
        for i, match_id in enumerate(match_ids):
            print(f"å¤„ç†æ¯”èµ› {i+1}/{len(match_ids)}: {match_id}")
            
            try:
                sample_text = self.generate_training_sample(match_id)
                sample_data = {
                    'match_id': match_id,
                    'training_text': sample_text,
                    'metadata': {
                        'generated_at': datetime.now().isoformat(),
                        'data_sources': ['opendota', 'stratz', 'liquipedia', 'dem'],
                        'has_comments': len(self.get_featured_comments(match_id)) > 0
                    }
                }
                training_samples.append(sample_data)
                
            except Exception as e:
                print(f"è·³è¿‡æ¯”èµ› {match_id}: {e}")
                continue
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if output_file:
            self._save_training_data(training_samples, output_file)
        
        return training_samples
    
    def _format_training_text(self, match_data: Dict[str, Any], comments: List[CommentData]) -> str:
        """æ ¼å¼åŒ–è®­ç»ƒæ–‡æœ¬"""
        
        # åŸºç¡€æ¯”èµ›ä¿¡æ¯
        base_info = self._format_match_base_info(match_data)
        
        # è‹±é›„æ•°æ®
        hero_analysis = self._format_hero_analysis(match_data)
        
        # ç»æµæ•°æ®
        economy_analysis = self._format_economy_analysis(match_data)
        
        # è¯„è®ºåˆ†æ
        comments_analysis = self._format_comments_analysis(comments)
        
        # ç”Ÿæˆè®­ç»ƒæ–‡æœ¬
        training_text = f"""# åˆ€å¡”æ¯”èµ›æ·±åº¦åˆ†æ #{match_data['match_id']}

## æ¯”èµ›æ¦‚å†µ
{base_info}

## è‹±é›„åˆ†æ
{hero_analysis}

## ç»æµèµ°åŠ¿
{economy_analysis}

## ç¤¾åŒºæ´å¯Ÿ
{comments_analysis}

## AIç»¼åˆåˆ†æ
åŸºäºä»¥ä¸Šå¤šç»´åº¦æ•°æ®å’Œç¤¾åŒºè®¨è®ºï¼Œè¿™åœºæ¯”èµ›çš„å…³é”®æ´å¯Ÿï¼š

### æˆ˜æœ¯å±‚é¢
- å…³é”®å†³ç­–ç‚¹ï¼š{match_data.get('key_decisions', 'æš‚æ— æ•°æ®')}
- èƒœè´Ÿæ‰‹ï¼š{match_data.get('winning_factors', 'æš‚æ— æ•°æ®')}

### æŠ€æœ¯å±‚é¢
- å›¢é˜Ÿé…åˆï¼š{match_data.get('team_coordination', 'ä¸­ç­‰æ°´å¹³')}
- ä¸ªäººå‘æŒ¥ï¼š{match_data.get('individual_performance', 'æœ‰å¾…åˆ†æ')}

### ç¤¾åŒºè§‚ç‚¹æ€»ç»“
{self._summarize_community_views(comments)}

### å¯å­¦ä¹ çš„è¦ç‚¹
1. {match_data.get('learning_point_1', 'éœ€è¦æ›´å¤šæ•°æ®')}
2. {match_data.get('learning_point_2', 'éœ€è¦æ›´å¤šæ•°æ®')}
3. {match_data.get('learning_point_3', 'éœ€è¦æ›´å¤šæ•°æ®')}

### ç±»ä¼¼æ¯”èµ›æ¨è
æƒ³è¦æå‡åœ¨è¿™åœºæ¯”èµ›ä¸­å­¦åˆ°çš„æŠ€èƒ½ï¼Œå»ºè®®è§‚çœ‹ï¼š{match_data.get('similar_matches', 'æš‚æ— æ¨è')}
---
è®­ç»ƒæ ‡ç­¾: {match_data.get('tags', 'ç»å…¸æ¯”èµ›, æˆ˜æœ¯åˆ†æ, ç¤¾åŒºçƒ­è®®')}
"""
        
        return training_text.strip()
    
    def _format_match_base_info(self, match_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æ¯”èµ›åŸºç¡€ä¿¡æ¯"""
        return f"""
- **æ¯”èµ›æ—¶é•¿**: {match_data.get('duration', 'æœªçŸ¥')}
- **è·èƒœæ–¹**: {match_data.get('winner', 'æœªçŸ¥')}
- **æ¸¸æˆæ¨¡å¼**: {match_data.get('game_mode', 'æœªçŸ¥')}
- **å¼€å§‹æ—¶é—´**: {match_data.get('start_time', 'æœªçŸ¥')}
- **ä¸€è¡€æ—¶é—´**: {match_data.get('first_blood_time', 'æœªçŸ¥')}ç§’
- **æ¸¸æˆç‰ˆæœ¬**: {match_data.get('game_version', 'æœªçŸ¥')}
- **æœåŠ¡å™¨åŒºåŸŸ**: {match_data.get('region', 'æœªçŸ¥')}
"""
    
    def _format_hero_analysis(self, match_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–è‹±é›„åˆ†æ"""
        radiant_heroes = match_data.get('radiant_heroes', [])
        dire_heroes = match_data.get('dire_heroes', [])
        
        hero_info = f"""
**å¤©è¾‰æ–¹é˜µå®¹**: {', '.join(radiant_heroes) if radiant_heroes else 'æš‚æ— æ•°æ®'}
**å¤œé­‡æ–¹é˜µå®¹**: {', '.join(dire_heroes) if dire_heroes else 'æš‚æ— æ•°æ®'}
"""
        
        # æ·»åŠ è‹±é›„å…‹åˆ¶åˆ†æ
        if 'hero_matchups' in match_data:
            hero_info += f"\n**è‹±é›„å…‹åˆ¶å…³ç³»**: {match_data['hero_matchups']}"
        
        return hero_info
    
    def _format_economy_analysis(self, match_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ç»æµåˆ†æ"""
        return f"""
- **æ€»ç»æµå·®è·**: {match_data.get('net_worth_lead', 'æœªçŸ¥')}
- **å…³é”®è£…å¤‡æ—¶æœº**: {match_data.get('key_item_timings', 'æœªçŸ¥')}
- **ä¹°æ´»æƒ…å†µ**: {match_data.get('buyback_status', 'æœªçŸ¥')}
- **è‚‰å±±ç»æµ**: {match_data.get('roshan_economy', 'æœªçŸ¥')}
"""
    
    def _format_comments_analysis(self, comments: List[CommentData]) -> str:
        """æ ¼å¼åŒ–è¯„è®ºåˆ†æ"""
        if not comments:
            return "æš‚æ— ç²¾é€‰è¯„è®º"
        
        analysis = ""
        for i, comment in enumerate(comments[:5]):  # æœ€å¤šå–5æ¡è¯„è®º
            analysis += f"""
**ç”¨æˆ·{comment.author}** ({comment.likes}ğŸ‘):
"{comment.content}"
"""
        
        return analysis
    
    def _summarize_community_views(self, comments: List[CommentData]) -> str:
        """æ€»ç»“ç¤¾åŒºè§‚ç‚¹"""
        if not comments:
            return "ç¤¾åŒºè®¨è®ºè¾ƒå°‘ï¼Œéœ€è¦æ›´å¤šç”¨æˆ·å‚ä¸"
        
        total_likes = sum(comment.likes for comment in comments)
        avg_quality = 'é«˜è´¨é‡' if any(c.analysis_quality == 'high' for c in comments) else 'ä¸­ç­‰è´¨é‡'
        
        return f"ç¤¾åŒºè®¨è®ºæ´»è·ƒ({len(comments)}æ¡è¯„è®ºï¼Œ{total_likes}ä¸ªç‚¹èµ)ï¼Œæ•´ä½“è´¨é‡{avg_quality}ï¼Œè§‚ç‚¹å¤šå…ƒ"
    
    def _get_match_data_from_backend(self, match_id: str) -> Dict[str, Any]:
        """ä»åç«¯æ•°æ®åº“è·å–æ¯”èµ›æ•°æ®"""
        try:
            # æŸ¥è¯¢æ¯”èµ›åŸºç¡€ä¿¡æ¯
            match = Match.query.filter_by(match_id=match_id).first()
            if not match:
                raise ValueError(f"æ¯”èµ› {match_id} ä¸å­˜åœ¨")
            
            # æ„å»ºç»¼åˆæ•°æ®
            match_data = {
                'match_id': match_id,
                'teams': f"{match.radiant_name} vs {match.dire_name}" if match.radiant_name and match.dire_name else "æœªçŸ¥é˜Ÿä¼",
                'duration': f"{match.duration // 60}:{match.duration % 60:02d}" if match.duration else "æœªçŸ¥",
                'winner': "å¤©è¾‰" if match.radiant_win else "å¤œé­‡" if match.radiant_win is not None else "æœªçŸ¥",
                'game_mode': match.game_mode or "æœªçŸ¥",
                'start_time': match.start_time.isoformat() if match.start_time else "æœªçŸ¥",
                'first_blood_time': match.first_blood_time or 0,
                'game_version': match.game_version or "æœªçŸ¥",
                'region': match.region or "æœªçŸ¥",
                'radiant_heroes': self._get_match_heroes(match_id, True),
                'dire_heroes': self._get_match_heroes(match_id, False),
                'net_worth_lead': self._get_economy_analysis(match_id),
                'key_decisions': self._analyze_key_decisions(match),
                'tags': self._generate_match_tags(match)
            }
            
            return match_data
            
        except Exception as e:
            print(f"ä»åç«¯è·å–æ¯”èµ›æ•°æ®å¤±è´¥ {match_id}: {e}")
            return self._get_mock_match_data(match_id)
    
    def _get_comments_from_backend(self, match_id: str, min_likes: int) -> List[CommentData]:
        """ä»åç«¯æ•°æ®åº“è·å–ç²¾é€‰è¯„è®º"""
        try:
            # æŸ¥è¯¢ç²¾é€‰è¯„è®º
            comments = Comment.query.filter_by(
                match_id=match_id,
                is_featured=True
            ).filter(Comment.likes >= min_likes).order_by(Comment.likes.desc()).limit(10).all()
            
            comment_data = []
            for comment in comments:
                comment_data.append(CommentData(
                    id=str(comment.id),
                    content=comment.content,
                    author=comment.author.username if comment.author else "åŒ¿åç”¨æˆ·",
                    created_at=comment.created_at,
                    likes=comment.likes or 0,
                    is_featured=comment.is_featured or False,
                    analysis_quality=comment.analysis_quality
                ))
            
            return comment_data
            
        except Exception as e:
            print(f"ä»åç«¯è·å–è¯„è®ºå¤±è´¥ {match_id}: {e}")
            return self._get_mock_comments(match_id)
    
    def _get_mock_match_data(self, match_id: str) -> Dict[str, Any]:
        """è·å–æ¨¡æ‹Ÿæ¯”èµ›æ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        return {
            'match_id': match_id,
            'teams': 'Team Secret vs OG',
            'duration': '45:30',
            'winner': 'å¤©è¾‰',
            'game_mode': 'é˜Ÿé•¿æ¨¡å¼',
            'start_time': '2024-01-15T14:30:00Z',
            'first_blood_time': 120,
            'game_version': '7.34e',
            'region': 'æ¬§æ´²è¥¿éƒ¨',
            'radiant_heroes': ['æ–§ç‹', 'å½±é­”', 'å¸•å…‹', 'æ°´æ™¶å®¤å¥³', 'å¤ä»‡ä¹‹é­‚'],
            'dire_heroes': ['å¹½é¬¼', 'ç¥ˆæ±‚è€…', 'æ½®æ±çŒäºº', 'å¤©æ€’æ³•å¸ˆ', 'æ’¼åœ°è€…'],
            'net_worth_lead': '15000é‡‘å¸ä¼˜åŠ¿',
            'key_item_timings': 'å½±é­”15åˆ†é’Ÿé»‘çš‡æ–ï¼Œæ–§ç‹18åˆ†é’Ÿè·³åˆ€',
            'buyback_status': 'å¤œé­‡æ–¹ä¹°æ´»CDï¼Œå¤©è¾‰æ–¹ä¹°æ´»å°±ç»ª',
            'roshan_economy': 'å¤©è¾‰æ–¹æ§åˆ¶è‚‰å±±ï¼Œè·å¾—ä¸æœ½ç›¾å’Œå¥¶é…ª',
            'key_decisions': '20åˆ†é’Ÿè‚‰å±±å›¢æˆ˜äº‰å¤ºï¼Œ35åˆ†é’Ÿé«˜åœ°æ¨è¿›',
            'winning_factors': 'ä¼˜ç§€çš„å›¢æˆ˜é…åˆå’Œå…³é”®è£…å¤‡æ—¶æœº',
            'team_coordination': 'ä¼˜ç§€',
            'individual_performance': 'å½±é­”å®Œç¾å‘æŒ¥',
            'learning_point_1': 'æ§åˆ¶è‚‰å±±æ—¶æœºçš„é‡è¦æ€§',
            'learning_point_2': 'æ ¸å¿ƒè£…å¤‡timingå¯¹å›¢æˆ˜çš„å½±å“',
            'learning_point_3': 'é«˜åœ°æ¨è¿›çš„æ—¶æœºé€‰æ‹©',
            'similar_matches': 'æ¨èè§‚çœ‹TI10æ€»å†³èµ›',
            'tags': 'ç»å…¸æ¯”èµ›, æˆ˜æœ¯åˆ†æ, å›¢æˆ˜é…åˆ, è£…å¤‡timing'
        }
    
    def _get_mock_comments(self, match_id: str) -> List[CommentData]:
        """è·å–æ¨¡æ‹Ÿè¯„è®ºæ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        return [
            CommentData(
                id='1',
                content='è¿™åœºæ¯”èµ›çš„å›¢æˆ˜é…åˆå¤ªç²¾å½©äº†ï¼å½±é­”çš„å®Œç¾å‘æŒ¥æ˜¯å…³é”®ã€‚',
                author='åˆ€å¡”åˆ†æå¸ˆ',
                created_at=datetime.now(),
                likes=25,
                is_featured=True,
                analysis_quality='high'
            ),
            CommentData(
                id='2',
                content='æ–§ç‹çš„è·³åˆ€æ—¶æœºæŠŠæ¡å¾—éå¸¸å¥½ï¼Œæ¯æ¬¡éƒ½èƒ½æŠ“åˆ°å…³é”®äººç‰©ã€‚',
                author='æˆ˜æœ¯å¤§å¸ˆ',
                created_at=datetime.now(),
                likes=18,
                is_featured=True,
                analysis_quality='high'
            ),
            CommentData(
                id='3',
                content='OGè¿™è¾¹çš„å¹½é¬¼å‘è‚²å¤ªæ…¢äº†ï¼Œæ²¡èƒ½åŠæ—¶å‚å›¢ã€‚',
                author='carryç©å®¶',
                created_at=datetime.now(),
                likes=12,
                is_featured=True,
                analysis_quality='medium'
            )
        ]
    
    def _get_match_heroes(self, match_id: str, is_radiant: bool) -> List[str]:
        """è·å–æ¯”èµ›è‹±é›„æ•°æ®ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“è·å–å®é™…è‹±é›„æ•°æ®
        if is_radiant:
            return ['æ–§ç‹', 'å½±é­”', 'å¸•å…‹', 'æ°´æ™¶å®¤å¥³', 'å¤ä»‡ä¹‹é­‚']
        else:
            return ['å¹½é¬¼', 'ç¥ˆæ±‚è€…', 'æ½®æ±çŒäºº', 'å¤©æ€’æ³•å¸ˆ', 'æ’¼åœ°è€…']
    
    def _get_economy_analysis(self, match_id: str) -> str:
        """è·å–ç»æµåˆ†æï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        return "15000é‡‘å¸ä¼˜åŠ¿"
    
    def _analyze_key_decisions(self, match) -> str:
        """åˆ†æå…³é”®å†³ç­–ç‚¹ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        return "20åˆ†é’Ÿè‚‰å±±å›¢æˆ˜äº‰å¤ºï¼Œ35åˆ†é’Ÿé«˜åœ°æ¨è¿›"
    
    def _generate_match_tags(self, match) -> str:
        """ç”Ÿæˆæ¯”èµ›æ ‡ç­¾ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        return "ç»å…¸æ¯”èµ›, æˆ˜æœ¯åˆ†æ, å›¢æˆ˜é…åˆ, è£…å¤‡timing"
    
    def _generate_error_sample(self, match_id: str, error_msg: str) -> str:
        """ç”Ÿæˆé”™è¯¯æ ·æœ¬"""
        return f"""# æ¯”èµ›æ•°æ®åˆ†æ #{match_id}

## é”™è¯¯ä¿¡æ¯
æ— æ³•è·å–æ¯”èµ›æ•°æ®: {error_msg}

## å»ºè®®
è¯·æ£€æŸ¥æ¯”èµ›IDæ˜¯å¦æ­£ç¡®ï¼Œæˆ–ç¨åé‡è¯•ã€‚
"""
    
    def _save_training_data(self, training_samples: List[Dict[str, Any]], output_file: str):
        """ä¿å­˜è®­ç»ƒæ•°æ®åˆ°æ–‡ä»¶"""
        try:
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(training_samples, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            print(f"å…±ç”Ÿæˆ {len(training_samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
            
        except Exception as e:
            print(f"ä¿å­˜è®­ç»ƒæ•°æ®å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•° - æµ‹è¯•è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨"""
    print("ğŸš€ å¯åŠ¨AIè®­ç»ƒæ•°æ®ç”Ÿæˆå™¨æµ‹è¯•...")
    
    # åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹ï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰
    generator = AITrainingDataGenerator(use_backend=False)
    
    # æµ‹è¯•å•ä¸ªæ¯”èµ›
    test_match_id = "1234567890"
    print(f"\nğŸ“Š ç”Ÿæˆæ¯”èµ› {test_match_id} çš„è®­ç»ƒæ–‡æœ¬...")
    
    training_text = generator.generate_training_sample(test_match_id)
    print("\n" + "="*50)
    print("ç”Ÿæˆçš„è®­ç»ƒæ–‡æœ¬:")
    print("="*50)
    print(training_text)
    print("="*50)
    
    # æµ‹è¯•æ‰¹é‡ç”Ÿæˆ
    print("\nğŸ“š æ‰¹é‡ç”Ÿæˆè®­ç»ƒæ•°æ®...")
    test_match_ids = ["1234567890", "0987654321", "1122334455"]
    
    output_file = "c:/Users/yb/PycharmProjects/PythonProject/ai-models/data/training_samples.json"
    training_samples = generator.generate_batch_training_data(test_match_ids, output_file)
    
    print(f"\nâœ… æ‰¹é‡ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {len(training_samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    total_chars = sum(len(sample['training_text']) for sample in training_samples)
    avg_chars = total_chars / len(training_samples) if training_samples else 0
    
    print(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - æ€»å­—ç¬¦æ•°: {total_chars:,}")
    print(f"   - å¹³å‡å­—ç¬¦æ•°: {avg_chars:.0f}")
    print(f"   - åŒ…å«è¯„è®ºçš„æ ·æœ¬: {sum(1 for s in training_samples if s['metadata']['has_comments'])}/{len(training_samples)}")


if __name__ == '__main__':
    main()